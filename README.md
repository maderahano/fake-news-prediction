# Predictive Analysis: Fake News Prediction
By: Made Rahano Satryani Widhi

| | Deskripsi |
| ----------- | ----------- |
| Dataset | [Fake News Prediction Dataset](https://www.kaggle.com/datasets/rajatkumar30/fake-news/data) |
| Masalah | Berita palsu atau berita hoax merupakan informasi yang disebarkan dengan sengaja untuk menyesatkan pembaca atau pendengar dengan tujuan tertentu, seperti memengaruhi opini publik atau menciptakan kebingungan. Proyek *Fake News Prediction* ini bertujuan untuk mengatasi masalah penyebaran berita palsu atau hoax yang semakin meresahkan masyarakat. Berita palsu memiliki potensi untuk mempengaruhi opini publik, memicu ketidakpercayaan terhadap media, serta memperburuk polarisasi dalam masyarakat. Masalah utama yang terjadi adalah sulitnya membedakan antara berita yang sah dan palsu karena kemampuan para pembuat berita palsu dalam menyusun konten yang menyerupai berita asli. Hal ini menimbulkan kebingungan di kalangan pembaca, serta menyebabkan informasi yang tidak benar tersebar luas dengan cepat melalui platform online. Dampak negatif dari penyebaran berita palsu ini sangat beragam, mulai dari mengganggu stabilitas politik hingga merugikan reputasi individu atau institusi yang menjadi sasaran tuduhan palsu. Oleh karena itu, pengembangan model prediksi berita palsu sangatlah penting untuk membantu mengidentifikasi dan memfilter konten yang tidak akurat atau menyesatkan, sehingga masyarakat dapat mengakses informasi yang lebih dapat dipercaya dan berkontribusi pada keberlangsungan demokrasi yang sehat. <br>- <br>-  *Dataset Fake News Prediction* berisi 6335 data dengan empat kolom yang relevan. Kolom pertama adalah nomor indeks, yang mengidentifikasi setiap data secara unik. Kolom kedua adalah *"Title"*, yang berisi judul dari setiap berita dalam dataset. Kolom ketiga adalah *"Text"*, yang memuat isi atau konten dari setiap berita. Terakhir, kolom keempat adalah *"Label"*, yang memberikan klasifikasi apakah berita tersebut dianggap sebagai *"Fake"* atau *"Real"*. Hubungan antara isi dataset dengan deskripsi masalah sebelumnya sangat erat, karena dataset ini menjadi dasar bagi pengembangan model prediksi berita palsu. Dengan menggunakan informasi dari judul dan teks berita, serta label yang sudah ditentukan, model dapat dilatih untuk mengidentifikasi pola-pola yang mengindikasikan keaslian atau ketidak-keaslian suatu berita. Dengan demikian, dataset ini merupakan bahan penting dalam upaya mengatasi masalah penyebaran berita palsu yang telah dijelaskan sebelumnya. |
| Solusi machine learning | Solusi machine learning yang dibuat adalah sebuah model klasifikasi yang menggunakan algoritma untuk membedakan antara berita asli dan palsu berdasarkan fitur-fitur tertentu seperti pola-pola teks yang terkait dengan berita palsu. Model ini dapat membantu secara otomatis mengidentifikasi dan memfilter berita palsu, memungkinkan pengguna untuk mengakses informasi yang lebih akurat dan dapat dipercaya. |
| Metode pengolahan |   - Proses *Data Ingestion* dilakukan melalui komponen CsvExampleGen dari TFX (*TensorFlow Extended*). Pada tahap ini, data diambil dari sumber (yang merupakan direktori DATA_ROOT yaitu '/content/data') dan dibagi menjadi dua bagian utama: *"train"* dan *"eval"*. Setiap bagian dibagi menggunakan hashing dengan masing-masing 8 dan 2 *bucket* untuk *train* dan *eval*.<br>- Proses *Data Validation* dilakukan melalui tiga tahap utama yaitu *Summary Statistic*, *Data Schema*, dan *Identify Anomaly on Dataset* untuk memastikan kualitasnya sebelum digunakan dalam proses Machine Learning. Tahap pertama adalah *Summary Statistic*, tahap pertama ini melibatkan pengumpulan statistik ringkasan dari dataset menggunakan komponen *StatisticsGen*. Ini membantu untuk mendapatkan pemahaman yang lebih baik tentang distribusi data, nilai-nilai rata-rata, dan informasi statistik lainnya yang relevan. Setelah itu, statistik yang dihasilkan ditampilkan untuk analisis lebih lanjut. Tahap kedua adalah *Data Schema*, tahap kedua ini melibatkan pembangunan skema data menggunakan komponen *SchemaGen*. Skema ini digunakan untuk menentukan struktur yang diharapkan dari dataset, termasuk tipe data dan batasan lainnya. Skema yang dihasilkan kemudian ditampilkan untuk pemeriksaan lebih lanjut. Terakhir, *Identify Anomaly on Dataset* adalah mengidentifikasi anomali dalam dataset menggunakan komponen ExampleValidator. Ini membandingkan statistik dataset dengan skema yang telah ditentukan untuk mendeteksi ketidaksesuaian atau anomali yang mungkin ada.<br>- Proses *Data Preprocessing* dilakukan menggunakan *TensorFlow Transform* (TFT) untuk mempersiapkan data mentah menjadi format yang cocok untuk proses pelatihan model. Modul *fake_news_transform.py* berisi fungsi preprocessing yang meliputi beberapa langkah. Pertama, terdapat fungsi untuk mengonversi label menjadi representasi *one-hot vector*, yang membantu dalam klasifikasi berita sebagai *"Fake"* atau *"Real"*. Selanjutnya, teks dari berita diubah menjadi huruf kecil untuk konsistensi dalam pemrosesan. Terakhir, nilai-nilai label diubah menjadi tipe data integer. Setelah semua langkah preprocessing selesai, proses transformasi diterapkan ke dataset menggunakan komponen Transform dari TFX, yang menghasilkan dataset yang telah diproses sesuai dengan aturan yang telah ditentukan.<br>- Proses *Model Development* dilakukan melalui beberapa langkah yang penting untuk melatih dan mengevaluasi model klasifikasi berita palsu. Pertama, ada pembuatan fungsi *'input_fn'* yang bertanggung jawab untuk menghasilkan dataset yang diproses sesuai dengan spesifikasi fitur yang telah ditentukan. Selanjutnya, dibangunlah arsitektur model yang terdiri dari beberapa lapisan seperti lapisan vektorisasi teks, lapisan embedding, dan lapisan konvolusi. Model ini kemudian dikompilasi dengan fungsi loss dan optimizer yang sesuai. Setelah itu, model dilatih menggunakan dataset yang sudah dipersiapkan sebelumnya dengan memanfaatkan *TensorBoard* untuk pemantauan dan callback *EarlyStopping* serta *ModelCheckpoint* untuk pengelolaan pembelajaran. Terakhir, model yang telah dilatih disimpan dalam format TensorFlow untuk digunakan dalam proses penyajian selanjutnya. Semua tahap ini terintegrasi dalam komponen Trainer dari TFX, yang mengelola proses pelatihan dan evaluasi model dengan menggunakan data yang telah diolah sebelumnya.<br>- Proses *Model Analysis & Validation* dilakukan untuk mengevaluasi dan memvalidasi model klasifikasi berita palsu yang telah dilatih sebelumnya. Tahap pertama melibatkan *Resolver*, yang bertanggung jawab untuk menentukan model terbaru yang telah disetujui untuk digunakan. Selanjutnya, konfigurasi evaluasi (eval_config) disiapkan dengan menetapkan spesifikasi model, spesifikasi pemotongan, dan metrik yang akan dievaluasi. *Evaluator* kemudian digunakan untuk mengevaluasi model yang dilatih sebelumnya dengan menggunakan data yang telah diproses sebelumnya, bersama dengan model dasar yang dipilih oleh *Resolver*. Hasil evaluasi kemudian divisualisasikan menggunakan *TensorFlow Model Analysis* (TFMA), termasuk metrik-metrik evaluasi seperti *Binary Accuracy* dan indikator keseimbangan yang ditampilkan dalam *widget*. Terakhir, model yang telah dievaluasi dan divalidasi dengan baik akan dipublikasikan menggunakan komponen Pusher ke destinasi yang telah ditentukan, sehingga siap digunakan dalam proses penyajian lebih lanjut. |
| Arsitektur model | Arsitektur model dimulai dengan lapisan vektorisasi teks untuk mengubah teks menjadi representasi numerik, diikuti oleh lapisan *embedding* yang mengonversi token-token tersebut menjadi vektor berdimensi rendah. Kemudian, terdapat dua lapisan konvolusi 1D untuk mengekstrak fitur-fitur dari urutan token, diikuti oleh lapisan *pooling global* maksimum untuk merangkum informasi yang paling penting dari hasil konvolusi. Selanjutnya, ada lapisan dense yang menghubungkan hasil pooling ke dalam jaringan saraf, diikuti oleh lapisan dropout untuk mencegah *overfitting*. Akhirnya, model menghasilkan output dengan lapisan *dense* dengan fungsi aktivasi sigmoid untuk memprediksi keaslian berita (*fake* atau *real*). Dengan optimisasi menggunakan *Adam* dan pengukuran kinerja menggunakan *binary crossentropy* dan akurasi biner, model ini siap untuk dilatih untuk tujuan klasifikasi berita. |
| Metrik evaluasi | Dalam konfigurasi evaluasi *TensorFlow Model Analysis* (TFMA) yang telah dibuat, metrik *Binary Accuracy* digunakan untuk mengevaluasi performa model. Metrik ini mengukur sejauh mana model dapat memprediksi label yang benar secara biner. Sebuah *threshold* ditetapkan untuk metrik ini, dimana jika nilai akurasi biner melebihi 0.5, dianggap baik. Selain itu, *Generic Change Threshold* digunakan untuk menetapkan ambang batas perubahan yang signifikan dalam performa model dari iterasi sebelumnya, dengan nilai absolut 0.0001. Pendekatan ini memungkinkan pemantauan yang lebih terperinci terhadap perubahan performa model, membantu dalam pemantauan dan peningkatan model secara efektif. |
| Performa model | Hasil evaluasi menunjukkan bahwa model yang telah dibuat menunjukkan performa yang baik. Metrik utama, *Binary Accuracy*, mencapai nilai 0.9015, yang menunjukkan kemampuan model untuk memprediksi label dengan akurasi yang tinggi. Selain itu, nilai *Loss* sebesar 0.7867 juga memberikan indikasi yang baik tentang sejauh mana model berhasil mengoptimalkan fungsi kerugian selama pelatihan. Evaluasi ini memberikan gambaran positif tentang kemampuan model dalam mengklasifikasikan berita asli atau palsu, dan hasilnya dapat diandalkan untuk pengambilan keputusan lebih lanjut terkait performa model tersebut. |
